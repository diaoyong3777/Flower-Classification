from base import *
from config import training_config
# import torch.optim as optim
import copy


def progressive_unfreezing():
    """æ¸è¿›å¼è§£å†»ï¼šä»è¾“å‡ºå±‚å¼€å§‹ï¼Œé€æ­¥è§£å†»æ›´å¤šå±‚"""
    # é‡æ–°åˆå§‹åŒ–æ¨¡å‹ï¼ˆç¡®ä¿ä»é¢„è®­ç»ƒæƒé‡å¼€å§‹ï¼‰
    model_ft, input_size = initialize_model(training_config.model_name, 102,
                                            feature_extract=True, use_pretrained=True)
    model_ft = model_ft.to(training_config.device)

    # åˆå§‹çŠ¶æ€ï¼šåªè®­ç»ƒè¾“å‡ºå±‚
    for param in model_ft.parameters():
        param.requires_grad = False
    for param in model_ft.fc.parameters():
        param.requires_grad = True

    # å®šä¹‰è§£å†»è®¡åˆ’ï¼ˆé’ˆå¯¹ResNet50ä¼˜åŒ–ï¼‰
    # åœ¨ gradual_train.py ä¸­ä¿®æ”¹è§£å†»è®¡åˆ’
    unfreeze_schedule = [
        {'layers': ['fc'], 'epochs': 25, 'lr': 0.001},  # é˜¶æ®µ1ï¼šå……åˆ†è®­ç»ƒè¾“å‡ºå±‚
        {'layers': ['layer4'], 'epochs': 30, 'lr': 0.0005},  # é˜¶æ®µ2ï¼š+layer4
        {'layers': ['layer3'], 'epochs': 25, 'lr': 0.0003},  # é˜¶æ®µ3ï¼š+layer3
        {'layers': ['layer2'], 'epochs': 20, 'lr': 0.0002},  # é˜¶æ®µ4ï¼š+layer2
        {'layers': ['layer1'], 'epochs': 15, 'lr': 0.0001},  # é˜¶æ®µ5ï¼š+layer1
        {'layers': ['bn1', 'conv1'], 'epochs': 10, 'lr': 0.00005},  # é˜¶æ®µ6ï¼šå…¨éƒ¨

        # æ–°å¢é˜¶æ®µ7ï¼šæ•´ä½“å¾®è°ƒ
        {'layers': ['all'], 'epochs': 20, 'lr': 0.00001},  # é˜¶æ®µ7ï¼šæ•´ä½“å¾®è°ƒ
    ]

    # å…¨å±€æœ€ä½³æ¨¡å‹å’Œå‡†ç¡®ç‡
    global_best_acc = 0
    global_best_model_wts = copy.deepcopy(model_ft.state_dict())
    best_stage = 0

    for stage_idx, stage in enumerate(unfreeze_schedule):
        print(f"\n{'=' * 60}")
        print(f"ğŸ“‹ é˜¶æ®µ {stage_idx + 1}/{len(unfreeze_schedule)}")
        print(f"ğŸ”„ è§£å†»å±‚: {stage['layers']}")
        print(f"â±ï¸ è®­ç»ƒè½®æ•°: {stage['epochs']}, å­¦ä¹ ç‡: {stage['lr']}")
        print(f"{'=' * 60}")

        # è§£å†»å½“å‰é˜¶æ®µçš„å±‚
        for layer_name in stage['layers']:
            for name, param in model_ft.named_parameters():
                if layer_name in name:
                    param.requires_grad = True
                    print(f"âœ… è§£å†»: {name}")

        # é…ç½®ä¼˜åŒ–å™¨ï¼ˆåªæ›´æ–°å¯è®­ç»ƒå‚æ•°ï¼‰
        trainable_params = []
        for param in model_ft.parameters():
            if param.requires_grad:
                trainable_params.append(param)

        # åœ¨ gradual_train.py ä¸­å°è¯•ä¸åŒçš„ä¼˜åŒ–å™¨
        def get_optimizer(model_params, lr):
            # æ–¹æ³•1: AdamW (æ¨è)
            return optim.AdamW(model_params, lr=lr, weight_decay=1e-4, betas=(0.9, 0.999))

            # æ–¹æ³•2: SGD with momentum
            # return optim.SGD(model_params, lr=lr, momentum=0.9, weight_decay=1e-4)

            # æ–¹æ³•3: Adam
            # return optim.Adam(model_params, lr=lr, weight_decay=1e-4)

        # ä½¿ç”¨
        optimizer_ft = get_optimizer(trainable_params, stage['lr'])
        scheduler = optim.lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.8)

        print(f"ğŸ”§ å¯è®­ç»ƒå‚æ•°æ•°é‡: {len(trainable_params)}")
        print(f"ğŸ¯ å½“å‰å­¦ä¹ ç‡: {stage['lr']}")

        # è®­ç»ƒå½“å‰é˜¶æ®µï¼Œå¹¶è·å–æœ€ä½³å‡†ç¡®ç‡
        stage_best_acc = global_best_acc  # ä¼ å…¥å½“å‰å…¨å±€æœ€ä½³ä½œä¸ºèµ·ç‚¹
        model_ft, acc_history, _, _, _, _ = train_model(
            model=model_ft,
            dataloaders=dataloaders,
            criterion=criterion,
            optimizer=optimizer_ft,
            scheduler=scheduler,
            num_epochs=stage['epochs'],
            best_acc=stage_best_acc  # ä¼ å…¥å½“å‰æœ€ä½³ï¼Œtrain_modelå†…éƒ¨ä¼šä¿å­˜æœ€ä½³æ¨¡å‹
        )

        # æ£€æŸ¥è¿™ä¸ªé˜¶æ®µæ˜¯å¦äº§ç”Ÿäº†æ–°çš„å…¨å±€æœ€ä½³
        if acc_history:
            current_stage_best = max([acc.item() for acc in acc_history])

            # å¦‚æœè¿™ä¸ªé˜¶æ®µäº§ç”Ÿäº†æ–°çš„å…¨å±€æœ€ä½³ï¼Œæ›´æ–°å…¨å±€æœ€ä½³æ¨¡å‹
            if current_stage_best > global_best_acc:
                global_best_acc = current_stage_best
                global_best_model_wts = copy.deepcopy(model_ft.state_dict())
                best_stage = stage_idx + 1
                print(f"ğŸ‰ å‘ç°æ–°çš„å…¨å±€æœ€ä½³! é˜¶æ®µ{best_stage}, å‡†ç¡®ç‡: {global_best_acc:.4f}")

                # ç«‹å³ä¿å­˜å½“å‰æœ€ä½³æ¨¡å‹
                best_state = {
                    'state_dict': global_best_model_wts,
                    'best_acc': global_best_acc,
                    'best_stage': best_stage,
                    'unfreeze_schedule': unfreeze_schedule
                }
                best_filename = f'gradual_best_stage{best_stage}_{global_best_acc:.4f}.pt'
                torch.save(best_state, best_filename)
                print(f"ğŸ’¾ æœ€ä½³æ¨¡å‹å·²ä¿å­˜: {best_filename}")
            else:
                print(f"ğŸ“Š é˜¶æ®µæœ€ä½³: {current_stage_best:.4f}, å…¨å±€æœ€ä½³: {global_best_acc:.4f} (é˜¶æ®µ{best_stage})")

    # è®­ç»ƒç»“æŸåï¼ŒåŠ è½½å…¨å±€æœ€ä½³æ¨¡å‹æƒé‡
    model_ft.load_state_dict(global_best_model_wts)
    print(f"\nğŸ” åŠ è½½å…¨å±€æœ€ä½³æ¨¡å‹ (é˜¶æ®µ{best_stage}, å‡†ç¡®ç‡: {global_best_acc:.4f})")

    # ä¿å­˜æœ€ç»ˆçš„æœ€ä½³æ¨¡å‹
    final_state = {
        'state_dict': global_best_model_wts,
        'best_acc': global_best_acc,
        'best_stage': best_stage,
        'unfreeze_schedule': unfreeze_schedule
    }
    final_filename = f'gradual_unfreeze_final_best_{global_best_acc:.4f}.pt'
    torch.save(final_state, final_filename)

    return model_ft, global_best_acc


# æ·»åŠ ç›´æ¥æ‰§è¡Œçš„åŠŸèƒ½
if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹æ¸è¿›å¼è§£å†»è®­ç»ƒ...")
    print(f"ğŸ“Š ä½¿ç”¨æ¨¡å‹: {training_config.model_name}")
    print(f"ğŸ¯ ç›®æ ‡ç±»åˆ«: 102ç±»èŠ±å‰")
    print(f"ğŸ–¼ï¸ å›¾åƒå°ºå¯¸: {training_config.Resize}x{training_config.Resize}")
    print(f"ğŸ“¦ æ‰¹æ¬¡å¤§å°: {training_config.batch_size}")
    print("ğŸ’¡ æ³¨æ„: å°†ä¿å­˜éªŒè¯é›†ç²¾åº¦æœ€é«˜çš„æ¨¡å‹")

    final_model, final_accuracy = progressive_unfreezing()

    print(f"\nğŸ‰ æ¸è¿›å¼è§£å†»è®­ç»ƒå®Œæˆï¼")
    print(f"ğŸ† æœ€ç»ˆæœ€ä½³å‡†ç¡®ç‡: {final_accuracy:.4f}")
    print(f"ğŸ’¾ æœ€ä½³æ¨¡å‹æ–‡ä»¶: gradual_unfreeze_final_best_{final_accuracy:.4f}.pt")
