# from base import *
#
# # # å°†æ‰€æœ‰å‚æ•°è§£å†», æ³¨æ„ï¼ï¼ï¼ï¼ï¼ï¼è¿™é‡Œä¼šæ˜¾ç¤ºåªè®­ç»ƒè¾“å‡ºå±‚ï¼Œä½†å…¶å®éƒ½è®­ç»ƒäº†
# # ã€æˆ–è€…åœ¨é…ç½®ä¸­è®¾ç½®Falseå³å¯,ä¸ºäº†æ–¹ä¾¿å¯ä»¥ç›´æ¥è¿™æ ·å…å¾—æ”¹ã€‚å¦‚æœè¦åªè®­ç»ƒè¾“å‡ºå±‚ï¼ŒæŠŠä¸‹é¢æ³¨é‡Šæ‰å°±è¡Œã€‘
# for param in model_ft.parameters():
#     param.requires_grad = True
#
#
# # åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
# checkpoint = torch.load(filename) # ä»æ–‡ä»¶è·å–æ¨¡å‹
# best_acc = checkpoint['best_acc'] # å°†ä¹‹å‰çš„æœ€ä½³å‡†ç¡®ç‡ä½œä¸ºåˆå§‹å€¼
# print(f"å½“å‰æ¨¡å‹å‡†ç¡®ç‡: {best_acc}")
# model_ft.load_state_dict(checkpoint['state_dict']) # åŠ è½½æ¨¡å‹
#
# # ä¸æ–­ä¼˜åŒ–
# train_model(best_acc=best_acc)


from base import *
from config import training_config
import copy

# é‡æ–°åˆå§‹åŒ–æ¨¡å‹ï¼Œç¡®ä¿ä¸æ¸è¿›å¼è®­ç»ƒæ—¶ç»“æ„ä¸€è‡´
model_ft, input_size = initialize_model(training_config.model_name, 102,
                                      feature_extract=False, use_pretrained=False)

# åŠ è½½æ¸è¿›å¼è®­ç»ƒçš„æœ€ä½³æ¨¡å‹
checkpoint = torch.load('pt/98%.pt')  # æ›¿æ¢ä¸ºä½ çš„å®é™…æ–‡ä»¶å
best_acc = checkpoint['best_acc']
best_stage = checkpoint.get('best_stage', 'unknown')
print(f"ğŸ“Š åŠ è½½æ¸è¿›å¼è®­ç»ƒæ¨¡å‹")
print(f"ğŸ¯ æœ€ä½³å‡†ç¡®ç‡: {best_acc:.4f}")
print(f"ğŸ“ˆ æœ€ä½³é˜¶æ®µ: {best_stage}")

# åŠ è½½æ¨¡å‹æƒé‡
model_ft.load_state_dict(checkpoint['state_dict'])
model_ft = model_ft.to(training_config.device)

# ===== é€‰æ‹©è®­ç»ƒç­–ç•¥ =====
strategy = "unfreeze_all"  # å¯é€‰: "unfreeze_all", "freeze_output", "custom_freeze"

if strategy == "unfreeze_all":
    # ç­–ç•¥1: è§£å†»æ‰€æœ‰å±‚ç»§ç»­è®­ç»ƒ
    print("ğŸ”“ è§£å†»æ‰€æœ‰å±‚ç»§ç»­è®­ç»ƒ")
    for param in model_ft.parameters():
        param.requires_grad = True

elif strategy == "freeze_output":
    # ç­–ç•¥2: å†»ç»“è¾“å‡ºå±‚ï¼Œè§£å†»å…¶ä»–å±‚ï¼ˆå¦‚æœæ€€ç–‘è¾“å‡ºå±‚è¿‡æ‹Ÿåˆï¼‰
    print("ğŸ”“ è§£å†»é™¤è¾“å‡ºå±‚å¤–çš„æ‰€æœ‰å±‚")
    for name, param in model_ft.named_parameters():
        if 'fc' in name:  # å†»ç»“è¾“å‡ºå±‚
            param.requires_grad = False
            print(f"â„ï¸ å†»ç»“å±‚: {name}")
        else:  # è§£å†»å…¶ä»–æ‰€æœ‰å±‚
            param.requires_grad = True

elif strategy == "custom_freeze":
    # ç­–ç•¥3: è‡ªå®šä¹‰å†»ç»“ç­–ç•¥ - åªè§£å†»æ·±å±‚ç½‘ç»œ
    print("ğŸ”“ è‡ªå®šä¹‰å†»ç»“: åªè§£å†»layer3å’Œlayer4")
    layers_to_unfreeze = ['layer3', 'layer4', 'fc']
    for name, param in model_ft.named_parameters():
        if any(layer in name for layer in layers_to_unfreeze):
            param.requires_grad = True
            print(f"âœ… è§£å†»: {name}")
        else:
            param.requires_grad = False
            print(f"â„ï¸ å†»ç»“: {name}")

# éªŒè¯å†»ç»“æ•ˆæœ
print("\nğŸ“‹ å¯è®­ç»ƒå‚æ•°ç»Ÿè®¡:")
trainable_count = 0
total_count = 0
for name, param in model_ft.named_parameters():
    total_count += 1
    if param.requires_grad:
        trainable_count += 1
        print(f"  âœ… {name}")

print(f"\nğŸ“Š æ€»å…±å‚æ•°å±‚æ•°: {total_count}")
print(f"ğŸ¯ å¯è®­ç»ƒå‚æ•°å±‚æ•°: {trainable_count}")
print(f"ğŸ“ˆ å¯è®­ç»ƒæ¯”ä¾‹: {trainable_count/total_count*100:.1f}%")

# é‡æ–°é…ç½®ä¼˜åŒ–å™¨ï¼Œåªæ›´æ–°å¯è®­ç»ƒå‚æ•°
params_to_update = []
for param in model_ft.parameters():
    if param.requires_grad:
        params_to_update.append(param)

# ä½¿ç”¨æ›´å°çš„å­¦ä¹ ç‡è¿›è¡Œå¾®è°ƒ
fine_tune_lr = training_config.learning_rate * 0.1  # åŸå§‹å­¦ä¹ ç‡çš„1/10
optimizer_ft = optim.Adam(params_to_update, lr=fine_tune_lr)
scheduler = optim.lr_scheduler.StepLR(optimizer_ft, step_size=training_config.step_size,
                                    gamma=training_config.gamma)

print(f"ğŸ”§ ä¼˜åŒ–å™¨å·²é‡æ–°é…ç½®")
print(f"ğŸ“š å­¦ä¹ ç‡: {fine_tune_lr}")

# ç»§ç»­è®­ç»ƒ
print("\nğŸš€ å¼€å§‹ç»§ç»­è®­ç»ƒ...")
train_model(model=model_ft,
           dataloaders=dataloaders,
           criterion=criterion,
           optimizer=optimizer_ft,
           scheduler=scheduler,
           best_acc=best_acc)

print("ğŸ‰ ç»§ç»­è®­ç»ƒå®Œæˆï¼")
